{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-Buw30tjD6h7g2Jpws5oGImIgDUlhB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building language model applications, especially those involving large language models (LLMs), but they serve different purposes and have distinct features. Here's an overview of their key differences:\\n\\n**1. Purpose and Focus:**\\n\\n- **LangChain:**\\n  - Primarily a framework for developing complex, multi-step applications that involve chaining together various components like prompts, models, memory, tools, and more.\\n  - Focuses on creating sophisticated conversational agents, workflows, or applications that require orchestration of multiple language model interactions.\\n  - Emphasizes modularity and flexibility, enabling developers to build pipelines with chaining, agents, and memory.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Focused on enabling easy ingestion, indexing, and querying of large unstructured data sources (like documents, PDFs, websites) tailored for LLMs.\\n  - Simplifies building semantic search and retrieval-augmented generation (RAG) applications by creating indices over data.\\n  - Designed to integrate with LLMs to answer questions based on external data sources rather than just prompts.\\n\\n**2. Core Functionality:**\\n\\n- **LangChain:**\\n  - Provides tools for prompt management, chaining multiple prompts or calls together.\\n  - Supports tools, memory, and agents to create dynamic and context-aware applications.\\n  - Offers integrations with various LLM providers, APIs, and data sources.\\n  - Suitable for building chatbots, synthesis pipelines, or applications involving complex LLM workflows.\\n\\n- **LlamaIndex:**\\n  - Offers data ingestion pipelines for various data formats.\\n  - Creates vector-based or graph-based indices for efficient retrieval.\\n  - Facilitates querying over large external datasets using prompts generated from the indices.\\n  - Primarily used to build QA systems, knowledge bases, or document assistants based on external data.\\n\\n**3. Use Cases:**\\n\\n- **LangChain:**\\n  - Conversational agents, chatbots, multi-step workflows, decision-making pipelines, automation.\\n  - Applications directing multiple LLM calls, prompts, or integrating with external tools.\\n\\n- **LlamaIndex:**\\n  - Semantic search, question answering over document collections.\\n  - Building knowledge bases that can be queried by LLMs using external document data.\\n\\n**4. Integration Approach:**\\n\\n- **LangChain:**\\n  - Modular components that can be combined in a variety of ways; supports custom logic and complex workflows.\\n  - Can incorporate retrieval modules, but primarily as part of broader application logic.\\n\\n- **LlamaIndex:**\\n  - Designed to bridge large datasets with LLMs via indexing and retrieval.\\n  - Often used as a retrieval component within a larger LangChain workflow, but its primary focus is data indexing and retrieval.\\n\\n---\\n\\n**In summary:**\\n\\n| Aspect                  | LangChain                                              | LlamaIndex                                                       |\\n|-------------------------|--------------------------------------------------------|------------------------------------------------------------------|\\n| Main Purpose            | Building complex LLM applications and workflows       | Indexing and querying large unstructured data for LLM answers  |\\n| Focus                   | Modular chaining, agents, prompts, memory             | Data ingestion, indexing, and retrieval                        |\\n| Typical Use Cases       | Chatbots, automation workflows, multi-step reasoning | Document Q&A, semantic search, knowledge bases                |\\n| Integration Style       | Orchestrator of components and tools                  | Data-focused retrieval components                               |\\n\\n**In practice, they can be complementary:**  \\nDevelopers might use LlamaIndex to manage and query large datasets and then incorporate it into LangChain workflows to build advanced applications.\\n\\n---\\n\\nIf you want a more tailored recommendation based on your specific project, feel free to share more details!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752909202, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=750, prompt_tokens=19, total_tokens=769, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications that leverage large language models (LLMs), but they serve different purposes and have distinct features. Here's a comparison to help clarify their differences:\n",
              "\n",
              "### Purpose and Focus\n",
              "- **LangChain**:\n",
              "  - Focuses on creating **conversational agents, chatbots, and multi-step workflows**.\n",
              "  - Provides tools for chaining together multiple language model calls, integrating external tools, memory management, and dialogue state tracking.\n",
              "  - Emphasizes building **complex applications** that require dynamic interactions, reasoning, and memory.\n",
              "\n",
              "- **LlamaIndex (GPT Index)**:\n",
              "  - Specializes in **building and querying large document corpora** using LLMs.\n",
              "  - Facilitates **indexing** of unstructured data (documents, PDFs, websites) into structures that can be efficiently queried.\n",
              "  - Designed to make **retrieval-augmented generation (RAG)** workflows easier, enabling LLMs to fetch relevant information from large datasets.\n",
              "\n",
              "### Core Functionality\n",
              "- **LangChain**:\n",
              "  - Provides **modules and abstractions** for prompts, models, chains, agents, tools, memory, and more.\n",
              "  - Supports **multi-modal workflows**: combining language models with APIs, databases, tools.\n",
              "  - Enables development of **personalized chatbots, autonomous agents** capable of performing complex tasks.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Offers **data ingestion pipelines** to load, parse, and index documents.\n",
              "  - Provides **querying mechanisms** that combine retrieval with language models to answer questions based on your data.\n",
              "  - Supports **custom indices** (e.g., tree indices, list indices) for specific data structures.\n",
              "\n",
              "### Use Cases\n",
              "- **LangChain**:\n",
              "  - Building conversational agents and chatbots.\n",
              "  - Automating workflows involving multiple steps and integrations.\n",
              "  - Developing autonomous agents that can reason and interact with external tools.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Creating a knowledge base from large collections of documents.\n",
              "  - Building question-answering systems that consult a corpus of data.\n",
              "  - Summarizing or analyzing large unstructured datasets.\n",
              "\n",
              "### Integration and Ecosystem\n",
              "- **LangChain**:\n",
              "  - Can work with various LLM providers (OpenAI, Hugging Face, etc.).\n",
              "  - Supports integrations with external APIs, databases, and tools.\n",
              "  - Has an active community and extensive documentation.\n",
              "\n",
              "- **LlamaIndex**:\n",
              "  - Designed to be compatible with multiple data sources and storage formats.\n",
              "  - Integrates with LLM APIs for querying and retrieval.\n",
              "  - Focused more on data management and retrieval workflows.\n",
              "\n",
              "### Summary\n",
              "| Aspect                | **LangChain**                                   | **LlamaIndex**                                 |\n",
              "|-----------------------|------------------------------------------------|------------------------------------------------|\n",
              "| Main Purpose          | Building chatbots, agents, workflows          | Indexing and querying large document corpora|\n",
              "| Core Focus            | Chain-of-thought, multi-step reasoning, tool integration | Data ingestion, search, retrieval          |\n",
              "| Use Cases             | Conversational AI, autonomous agents          | Knowledge bases, document QA                |\n",
              "| Data Handling         | Dynamic, multi-turn conversations              | Large unstructured datasets                   |\n",
              "\n",
              "---\n",
              "\n",
              "### In brief:\n",
              "- **Use LangChain** if you are developing complex conversational applications, workflows, or hybrid systems that involve reasoning, memory, and tool integration.\n",
              "- **Use LlamaIndex** if your goal is to build a system that indexes vast amounts of unstructured data and allows efficient retrieval using LLMs.\n",
              "\n",
              "Both frameworks can be complementary in some scenarios, and choosing between them depends on your specific application needs."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? After waiting forever for this question, I’d go nuts for some crushed ice right now! Cubed ice just doesn’t cut it when you’re starving and desperate for a cool, refreshing boost. Honestly, choose wisely—my stomach’s growling and I need something to satisfy this relentless hunger!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fun because it's great for summer drinks and adds a refreshing crunch! Cubed ice, on the other hand, looks sleek and melts more slowly, making it perfect for sipping. Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Buw7PzCEXly9zVPM6B7PVufI6NtAt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun because it's great for summer drinks and adds a refreshing crunch! Cubed ice, on the other hand, looks sleek and melts more slowly, making it perfect for sipping. Which do you prefer?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752909475, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=45, prompt_tokens=30, total_tokens=75, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a sentence using the words:\n",
              "\n",
              "\"During the workshop, she demonstrated a quick stimple to fix the broken device, while the team marveled at the rare falbean species they observed nearby.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly tightened the falbean bolt, ensuring everything was securely in place."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze both options step by step, assuming Billy wants to arrive before 7PM EDT.\n",
              "\n",
              "**Important details:**\n",
              "\n",
              "- **Current time:** 1PM local time.\n",
              "- **Target arrival time:** before 7PM EDT.\n",
              "\n",
              "---\n",
              "\n",
              "### Scenario 1: Flying + Bus\n",
              "- **Fly:** 3 hours.\n",
              "- **Then take a bus:** 2 hours.\n",
              "- **Total travel time:** 3 + 2 = **5 hours**.\n",
              "\n",
              "**Departure time:** Since Billy is starting at 1PM local time, he can depart immediately.\n",
              "\n",
              "**Arrival time:** 1PM + 5 hours = 6PM local time.\n",
              "\n",
              "**Is this before 7PM EDT?**  \n",
              "- Yes, if local time is aligned with EDT, he would arrive at **6PM EDT**, which is before 7PM.\n",
              "\n",
              "---\n",
              "\n",
              "### Scenario 2: Teleporter + Bus\n",
              "- **Teleporter:** 0 hours.\n",
              "- **Bus:** 1 hour.\n",
              "- **Total travel time:** 0 + 1 = **1 hour**.\n",
              "\n",
              "**Departure time:** at 1PM, same as above.\n",
              "\n",
              "**Arrival time:** 1PM + 1 hour = 2PM local time.\n",
              "\n",
              "**Is this before 7PM EDT?**  \n",
              "- Yes, 2PM is before 7PM.\n",
              "\n",
              "---\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "Both options get Billy home well before the 7PM cutoff, with the teleporter option arriving significantly earlier.\n",
              "\n",
              "**Does it matter which option Billy selects?**  \n",
              "- **In terms of arriving before 7PM**, no, both options are fine.\n",
              "- **In terms of arrival time:** the teleporter + bus gets him home earlier.\n",
              "- **In terms of resource or convenience:** depends on Billy's priorities, but travel time difference is clear.\n",
              "\n",
              "**Final answer:**  \n",
              "*No, it doesn't matter in terms of reaching before 7PM.* Both options allow Billy to arrive on time."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the options step by step.\n",
              "\n",
              "**Given Data:**\n",
              "- Current local time: 1PM\n",
              "- Billy wants to arrive **before 7PM EDT**\n",
              "- Time constraint: must arrive **before 7PM**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 1: Determine the total travel times for each option\n",
              "\n",
              "**Option 1: Fly + Bus**\n",
              "- Fly time: 3 hours\n",
              "- Bus time afterward: 2 hours\n",
              "- Total travel time: 3 + 2 = **5 hours**\n",
              "\n",
              "**Option 2: Teleporter + Bus**\n",
              "- Teleporter time: 0 hours\n",
              "- Bus time afterward: 1 hour\n",
              "- Total travel time: 0 + 1 = **1 hour**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 2: Calculate latest departure times to arrive before 7PM\n",
              "\n",
              "**Option 1:**\n",
              "- Arrival deadline: 7PM\n",
              "- Total travel time: 5 hours\n",
              "- Latest departure time: 7PM - 5 hours = **2PM**\n",
              "\n",
              "**Option 2:**\n",
              "- Arrival deadline: 7PM\n",
              "- Total travel time: 1 hour\n",
              "- Latest departure time: 7PM - 1 hour = **6PM**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 3: Compare current time to departure deadlines\n",
              "\n",
              "- Current local time: 1PM\n",
              "- For **Option 1 (Fly + Bus)**:\n",
              "  - Billy must depart **by 2PM**. Since it's currently 1PM, he has **1 hour** to decide and depart.\n",
              "- For **Option 2 (Teleporter + Bus)**:\n",
              "  - Billy must depart **by 6PM**. He has **5 hours** remaining.\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "Billy can still make either option, as he has enough time for both:\n",
              "\n",
              "- He needs to depart **by 2PM** for Option 1.\n",
              "- He has until **6PM** for Option 2.\n",
              "\n",
              "**Does it matter which option he chooses?**\n",
              "\n",
              "- From a timing perspective, **both options are feasible since current time is 1PM**.\n",
              "- The main difference is the duration of travel and flexibility:\n",
              "\n",
              "  - **Option 1** takes longer (5 hours total), and he must depart relatively soon.\n",
              "  - **Option 2** is much faster (1 hour), giving more flexibility.\n",
              "\n",
              "**Final Answer:**\n",
              "\n",
              "**It does matter, because taking the teleporter allows Billy to arrive well before 7PM without rushing, while flying requires him to depart soon (by 2PM).** \n",
              "\n",
              "In terms of ensuring timely arrival, **taking the teleporter + bus gives Billy more margin**, but **both options can work if he departs on time**. If Billy wants to guarantee arriving before 7PM without tight schedule constraints, the teleporter + bus is the better choice.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary:**  \n",
              "Yes, it does matter which option Billy selects, because the teleporter plus bus allows for more departure flexibility and guarantees earlier arrival, while the airplane plus bus requires a prompt departure."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
